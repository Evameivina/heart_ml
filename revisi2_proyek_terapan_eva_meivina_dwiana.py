# -*- coding: utf-8 -*-
"""Revisi2_Proyek Terapan_Eva Meivina Dwiana.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vG106rNABgvur__E_hDOyXgkTCkphavv

# Laporan Proyek Machine Learning - Eva Meivina Dwiana

## Import Library
Pada tahap awal, kita memuat pustaka (library) yang dibutuhkan untuk seluruh proses analisis dan pemodelan. Setiap pustaka memiliki fungsi khusus yang membantu dalam membaca data, preprocessing, pemodelan, hingga evaluasi performa model.

**Pustaka yang digunakan:**

* **pandas**: untuk membaca file CSV, menampilkan, dan memanipulasi data dalam bentuk DataFrame yang memudahkan analisis.

* **train_test_split** dari sklearn.model_selection: membagi data menjadi data latih dan data uji agar model dapat dilatih dan diuji secara terpisah untuk menghindari overfitting.

* **OneHotEncoder dan StandardScaler**: melakukan encoding data kategorikal menjadi numerik dan menormalkan data numerik agar memiliki skala seragam.

* **ColumnTransformer**: menggabungkan beberapa proses preprocessing (numerik dan kategorikal) dalam satu pipeline yang terstruktur.

* **LogisticRegression dan RandomForestClassifier**: dua algoritma klasifikasi yang akan digunakan untuk membangun model prediksi penyakit jantung.

* **GridSearchCV**: melakukan pencarian kombinasi hyperparameter terbaik untuk model Random Forest secara otomatis dan sistematis.

* **accuracy_score, precision_score, recall_score, f1_score, classification_report**: mengukur performa model dari berbagai sudut, seperti akurasi, ketepatan, dan kemampuan menangkap data positif.

Dengan memuat pustaka ini di awal, proses pengolahan data dan pemodelan dapat dilakukan secara efisien dan terstruktur.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

"""## Load Dataset

Dataset ini awalnya diunduh dari Kaggle dalam format file CSV, kemudian file tersebut diunggah ke repository GitHub untuk memudahkan akses dan kolaborasi. Dengan menaruh file di GitHub, kita bisa mengambil data secara langsung lewat URL raw GitHub tanpa perlu mengunduh ulang file secara manual setiap kali melakukan analisis.

**Mengapa proses ini dilakukan?**

* Mengunduh dari Kaggle memberikan dataset asli dan terpercaya untuk analisis.

* Mengunggah ke GitHub memungkinkan akses yang mudah dan cepat melalui URL langsung, sehingga kode dapat dijalankan di berbagai lingkungan tanpa repot memindahkan file.

* Memuat dataset menggunakan pd.read_csv() dari pandas mempermudah manipulasi data dalam bentuk DataFrame yang powerful untuk analisis dan pemodelan.

**Apa yang dilakukan?**

Fungsi pd.read_csv() membaca file CSV dari URL GitHub, memuat data ke dalam DataFrame bernama heart_df.

**Hasil proses:**

Data berhasil dimuat dengan struktur yang lengkap (918 baris, 12 kolom) dan siap untuk tahapan eksplorasi dan pemrosesan selanjutnya.
"""

url = "https://raw.githubusercontent.com/Evameivina/heart_ml/refs/heads/main/heart.csv"
heart_df = pd.read_csv(url)

"""## Data Understanding
Pada tahap ini, kita melakukan pemahaman awal terhadap dataset untuk mengenali struktur, isi, dan kualitas data yang akan dianalisis. Hal ini penting agar langkah selanjutnya dalam proses pengolahan dan pemodelan data bisa dilakukan dengan tepat dan efektif.

**Apa yang dilakukan?**

* Menampilkan 5 baris pertama data (head()) untuk melihat contoh nilai pada setiap kolom dan memastikan data sudah terbaca dengan benar.

* Memeriksa informasi tipe data dan jumlah nilai yang tidak kosong (info()) untuk mengetahui apakah ada missing value dan jenis data (numerik atau kategorikal).

* Menghitung statistik deskriptif dasar (describe()) untuk fitur numerik, yang memberikan gambaran umum seperti nilai rata-rata, standar deviasi, nilai minimum, dan maksimum.

* Mengecek duplikasi data untuk memastikan tidak ada data ganda yang bisa mengganggu analisis.

* Memeriksa missing values secara menyeluruh untuk memastikan data lengkap dan tidak ada nilai hilang.

* Mengamati distribusi target *HeartDisease* untuk memahami proporsi kelas positif dan negatif, yang penting untuk pemodelan klasifikasi.

**Mengapa dilakukan?**

Pemahaman yang baik terhadap data memastikan kita mengetahui karakteristik data, menemukan potensi masalah (seperti missing value atau duplikasi), dan memahami keseimbangan kelas target agar strategi pemodelan yang digunakan sesuai.

**Hasil temuan:**

* Dataset berisi 918 baris dan 12 kolom dengan tipe data campuran antara numerik dan kategorikal.

* Tidak ada missing values atau data duplikat, sehingga data sudah bersih untuk diproses lebih lanjut.

* Target *HeartDisease* memiliki distribusi sekitar 55% positif dan 45% negatif, yang relatif seimbang untuk pemodelan klasifikasi.


"""

# Menampilkan 5 data teratas untuk melihat isi dataset
print("5 data teratas:")
print(heart_df.head())

# Melihat informasi tipe data dan kelengkapan data
print("Info dataset:")
heart_df.info()

# Statistik deskriptif untuk fitur numerik
print("Statistik deskriptif:")
print(heart_df.describe())

# Mengecek data duplikat
print("Cek duplikat:")
print(f"Jumlah data duplikat: {heart_df.duplicated().sum()}")

# Mengecek missing values
print("Cek missing values:")
print(heart_df.isnull().sum())

# Melihat distribusi kelas target
print("Distribusi target HeartDisease:")
print(heart_df['HeartDisease'].value_counts(normalize=True))

"""### Data Preparation

Tahap ini bertujuan untuk menyiapkan data agar dapat diproses dan dilatih oleh model machine learning secara optimal. Berikut langkah-langkah yang dilakukan:

**1. Memisahkan Fitur (X) dan Target (y)**

Langkah pertama adalah memisahkan fitur independen (X) dan target (y). Hal ini penting agar model hanya belajar dari fitur yang relevan tanpa melihat target secara langsung.
"""

# Memisahkan fitur (X) dan target (y) untuk memudahkan proses pemodelan
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

"""**2. Mengidentifikasi Tipe Fitur: Numerik dan Kategorikal**

Fitur dataset dibagi menjadi dua tipe utama untuk penanganan yang berbeda:

* **Numerik:** fitur berupa angka yang membutuhkan standardisasi agar skalanya seragam.

* **Kategorikal:** fitur berupa kategori yang perlu diubah ke format numerik menggunakan one-hot encoding.
"""

# Mendefinisikan fitur numerik dan kategorikal
# Ini penting agar preprocessing bisa dilakukan secara tepat pada masing-masing tipe data
num_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']
cat_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']

"""**3. Menentukan Metode Preprocessing**

Untuk mengolah masing-masing tipe fitur, digunakan transformer berikut:

* StandardScaler untuk fitur numerik agar dinormalisasi ke skala standar (mean 0, std 1).

* OneHotEncoder untuk fitur kategorikal agar dikonversi menjadi fitur biner (dummy variables).
"""

# Menyiapkan transformer untuk fitur numerik: StandardScaler digunakan untuk normalisasi
num_transformer = StandardScaler()
# Menyiapkan transformer untuk fitur kategorikal: OneHotEncoder untuk ubah ke format numerik
cat_transformer = OneHotEncoder(handle_unknown='ignore')

"""**4.Menggabungkan Preprocessing dengan ColumnTransformer**

ColumnTransformer digunakan untuk menerapkan transformer berbeda pada kolom numerik dan kategorikal sekaligus dalam pipeline yang efisien.
"""

# Menggabungkan preprocessing numerik dan kategorikal menggunakan ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_transformer, num_features),
        ('cat', cat_transformer, cat_features)
    ])

"""**5. Membagi Data Menjadi Data Latih dan Uji**

Dataset dibagi menjadi data latih dan uji dengan perbandingan 80:20. Stratifikasi dilakukan berdasarkan target untuk menjaga proporsi kelas tetap seimbang di kedua subset.
"""

# Membagi dataset menjadi data latih (80%) dan data uji (20%)
# Stratifikasi digunakan agar proporsi kelas target seimbang di kedua subset
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

"""**6. Menerapkan Preprocessing ke Data**

Setelah mendefinisikan pipeline preprocessing (preprocessor), langkah berikutnya adalah menerapkannya ke data latih dan data uji.

* Pada data latih (X_train), kita menggunakan fit_transform().
Fungsi fit() akan menghitung parameter yang diperlukan (misal mean dan std untuk StandardScaler, atau kategori untuk OneHotEncoder) dari data latih, lalu transform() akan menerapkan transformasi tersebut.
Ini memastikan model belajar dari data latih saja, menghindari "data leakage".

* Pada data uji (X_test), kita hanya menggunakan transform() saja, tanpa fit().
Hal ini agar transformasi pada data uji konsisten dengan data latih yang sudah dipelajari sebelumnya.

Setelah preprocessing selesai, kita bisa melihat bentuk (dimensi) data hasil transformasi untuk memastikan semua fitur sudah diproses sesuai harapan.
"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print("Preprocessing selesai.")
print(f"Bentuk X_train_processed: {X_train_processed.shape}")
print(f"Bentuk X_test_processed: {X_test_processed.shape}")

"""**Hasil**

* Preprocessing selesai. menandakan bahwa seluruh proses transformasi fitur numerik dan kategorikal telah berhasil diterapkan tanpa error.

* Bentuk X_train_processed: (734, 20) berarti data latih yang sudah diproses terdiri dari 734 baris (sampel) dan 20 kolom (fitur hasil transformasi). Jumlah fitur ini lebih banyak dari fitur awal karena fitur kategorikal telah diubah menjadi beberapa kolom baru lewat one-hot encoding.

* Bentuk X_test_processed: (184, 20) berarti data uji yang sudah diproses terdiri dari 184 baris dan juga 20 kolom fitur, sama seperti data latih, sehingga model dapat bekerja konsisten pada kedua set data.

## Modeling

Membangun dan membandingkan berbagai model klasifikasi untuk memilih model dengan performa terbaik.

**Model yang Digunakan:**

* **Logistic Regression**

Model baseline sederhana untuk klasifikasi biner.

Alasannya dipilih:

* Cepat dan mudah digunakan.
* Memberikan baseline awal.
* Mudah diinterpretasi.
"""

# Model 1: Logistic Regression
# Membuat dan melatih model Logistic Regression dengan data yang sudah diproses untuk memprediksi penyakit jantung.
logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train_processed, y_train)

"""* **Random Forest (Default)**

Model ensemble berbasis decision tree.

Alasannya dipilih:
* Mampu menangani data non-linear.
* Lebih tahan terhadap outlier dan overfitting.

"""

# Random Forest
# Membuat dan melatih model Random Forest untuk memprediksi penyakit jantung menggunakan data yang sudah diproses.
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_processed, y_train)

"""**Random Forest (Tuned)**
Melakukan tuning hyperparameter menggunakan GridSearchCV agar model bekerja lebih optimal.

Parameter yang dituning:
* n_estimators (jumlah pohon)
* max_depth (kedalaman maksimal pohon)
* min_samples_split (minimal data untuk split cabang)
"""

# Tuning Random Forest dengan Grid Search
# Melakukan pencarian kombinasi parameter terbaik Random Forest menggunakan GridSearchCV agar model lebih optimal.
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid_search.fit(X_train_processed, y_train)

best_rf = grid_search.best_estimator_
print(f"Best parameters RF: {grid_search.best_params_}")

"""## Evaluation

**Tujuan**
Menilai performa masing-masing model secara obyektif dan membandingkan efektivitasnya dalam prediksi.

**Metrik Evaluasi:**

* **Accuracy:** proporsi prediksi yang benar.
* **Precision:** proporsi positif yang benar-benar positif.
* **Recall:** proporsi kasus positif yang berhasil dideteksi.
* **F1-score:** harmonisasi antara precision dan recall.

**Proses Evaluasi:**

Digunakan fungsi *evaluate_model()* untuk semua model, mencetak classification report dan menghitung metrik utama.
"""

# Fungsi ini menghitung dan menampilkan metrik evaluasi utama serta laporan klasifikasi dari model pada data uji.
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(classification_report(y_test, y_pred))
    return acc, prec, rec, f1

"""## Interpretasi Hasil Evaluasi Model

Logistic Regression memberikan hasil awal yang layak, namun kalah performa dibanding Random Forest. Random Forest default sudah cukup baik, tetapi setelah tuning hyperparameter, performa model meningkat terutama pada metrik recall dan F1-score. Hal ini menunjukkan bahwa model yang dituning lebih mampu mengenali data positif secara lebih akurat, yang sangat penting dalam kasus prediksi penyakit jantung.

"""

# Evaluasi performa model Logistic Regression pada data uji.
print("Logistic Regression Evaluation")
logreg_metrics = evaluate_model(logreg, X_test_processed, y_test)

# Evaluasi performa model Random Forest standar pada data uji.
print("Random Forest Evaluation")
rf_metrics = evaluate_model(rf, X_test_processed, y_test)

# Evaluasi performa model Random Forest hasil tuning parameter pada data uji.
print("Tuned Random Forest Evaluation")
tuned_rf_metrics = evaluate_model(best_rf, X_test_processed, y_test)

"""### Hasil Evaluasi (Test Set)

| Model                 | Accuracy | Precision | Recall | F1-score |
|-----------------------|----------|-----------|--------|----------|
| Logistic Regression   | 0.86     | 0.84      | 0.89   | 0.86     |
| Random Forest Default | 0.88     | 0.87      | 0.90   | 0.88     |
| Random Forest Tuned   | 0.91     | 0.90      | 0.93   | 0.91     |

**Hasil**

* Random Forest Tuned paling unggul dalam akurasi dan recall.

* Model cukup andal untuk deteksi penyakit jantung secara dini.
"""

