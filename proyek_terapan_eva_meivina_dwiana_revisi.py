# -*- coding: utf-8 -*-
"""Proyek Terapan_Eva Meivina Dwiana_Revisi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vG106rNABgvur__E_hDOyXgkTCkphavv

# Laporan Proyek Machine Learning - Eva Meivina Dwiana

## Import Library
Pada tahap awal, kita memuat pustaka (library) yang dibutuhkan untuk seluruh proses analisis dan pemodelan. Setiap pustaka memiliki fungsi khusus yang membantu dalam membaca data, preprocessing, pemodelan, hingga evaluasi performa model.

**Pustaka yang digunakan:**

* **pandas**: untuk membaca file CSV, menampilkan, dan memanipulasi data dalam bentuk DataFrame yang memudahkan analisis.

* **train_test_split** dari sklearn.model_selection: membagi data menjadi data latih dan data uji agar model dapat dilatih dan diuji secara terpisah untuk menghindari overfitting.

* **OneHotEncoder dan StandardScaler**: melakukan encoding data kategorikal menjadi numerik dan menormalkan data numerik agar memiliki skala seragam.

* **ColumnTransformer**: menggabungkan beberapa proses preprocessing (numerik dan kategorikal) dalam satu pipeline yang terstruktur.

* **LogisticRegression dan RandomForestClassifier**: dua algoritma klasifikasi yang akan digunakan untuk membangun model prediksi penyakit jantung.

* **GridSearchCV**: melakukan pencarian kombinasi hyperparameter terbaik untuk model Random Forest secara otomatis dan sistematis.

* **accuracy_score, precision_score, recall_score, f1_score, classification_report**: mengukur performa model dari berbagai sudut, seperti akurasi, ketepatan, dan kemampuan menangkap data positif.

Dengan memuat pustaka ini di awal, proses pengolahan data dan pemodelan dapat dilakukan secara efisien dan terstruktur.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

"""## Load Dataset

Dataset ini awalnya diunduh dari Kaggle dalam format file CSV, kemudian file tersebut diunggah ke repository GitHub untuk memudahkan akses dan kolaborasi. Dengan menaruh file di GitHub, kita bisa mengambil data secara langsung lewat URL raw GitHub tanpa perlu mengunduh ulang file secara manual setiap kali melakukan analisis.

**Mengapa proses ini dilakukan?**

* Mengunduh dari Kaggle memberikan dataset asli dan terpercaya untuk analisis.

* Mengunggah ke GitHub memungkinkan akses yang mudah dan cepat melalui URL langsung, sehingga kode dapat dijalankan di berbagai lingkungan tanpa repot memindahkan file.

* Memuat dataset menggunakan pd.read_csv() dari pandas mempermudah manipulasi data dalam bentuk DataFrame yang powerful untuk analisis dan pemodelan.

**Apa yang dilakukan?**

Fungsi pd.read_csv() membaca file CSV dari URL GitHub, memuat data ke dalam DataFrame bernama heart_df.

**Hasil proses:**

Data berhasil dimuat dengan struktur yang lengkap (918 baris, 12 kolom) dan siap untuk tahapan eksplorasi dan pemrosesan selanjutnya.
"""

url = "https://raw.githubusercontent.com/Evameivina/heart_ml/refs/heads/main/heart.csv"
heart_df = pd.read_csv(url)

"""## Data Understanding
Pada tahap ini, kita melakukan pemahaman awal terhadap dataset untuk mengenali struktur, isi, dan kualitas data yang akan dianalisis. Hal ini penting agar langkah selanjutnya dalam proses pengolahan dan pemodelan data bisa dilakukan dengan tepat dan efektif.

**Apa yang dilakukan?**

* Menampilkan 5 baris pertama data (head()) untuk melihat contoh nilai pada setiap kolom dan memastikan data sudah terbaca dengan benar.

* Memeriksa informasi tipe data dan jumlah nilai yang tidak kosong (info()) untuk mengetahui apakah ada missing value dan jenis data (numerik atau kategorikal).

* Menghitung statistik deskriptif dasar (describe()) untuk fitur numerik, yang memberikan gambaran umum seperti nilai rata-rata, standar deviasi, nilai minimum, dan maksimum.

* Mengecek duplikasi data untuk memastikan tidak ada data ganda yang bisa mengganggu analisis.

* Memeriksa missing values secara menyeluruh untuk memastikan data lengkap dan tidak ada nilai hilang.

* Mengamati distribusi target *HeartDisease* untuk memahami proporsi kelas positif dan negatif, yang penting untuk pemodelan klasifikasi.

**Mengapa dilakukan?**

Pemahaman yang baik terhadap data memastikan kita mengetahui karakteristik data, menemukan potensi masalah (seperti missing value atau duplikasi), dan memahami keseimbangan kelas target agar strategi pemodelan yang digunakan sesuai.

**Hasil temuan:**

* Dataset berisi 918 baris dan 12 kolom dengan tipe data campuran antara numerik dan kategorikal.

* Tidak ada missing values atau data duplikat, sehingga data sudah bersih untuk diproses lebih lanjut.

* Target *HeartDisease* memiliki distribusi sekitar 55% positif dan 45% negatif, yang relatif seimbang untuk pemodelan klasifikasi.


"""

# Menampilkan 5 data teratas untuk melihat isi dataset
print("5 data teratas:")
print(heart_df.head())

# Melihat informasi tipe data dan kelengkapan data
print("Info dataset:")
heart_df.info()

# Statistik deskriptif untuk fitur numerik
print("Statistik deskriptif:")
print(heart_df.describe())

# Mengecek data duplikat
print("Cek duplikat:")
print(f"Jumlah data duplikat: {heart_df.duplicated().sum()}")

# Mengecek missing values
print("Cek missing values:")
print(heart_df.isnull().sum())

# Melihat distribusi kelas target
print("Distribusi target HeartDisease:")
print(heart_df['HeartDisease'].value_counts(normalize=True))

"""### Data Preparation

Tahap ini bertujuan untuk menyiapkan data dalam bentuk yang sesuai untuk pelatihan model.

* Memisahkan fitur (X) dan target (y) agar model hanya belajar dari fitur yang relevan.

* Mengelompokkan fitur numerik dan kategorikal, karena masing-masing jenis fitur membutuhkan teknik praproses yang berbeda.

* Normalisasi fitur numerik menggunakan StandardScaler agar semua fitur numerik berada dalam skala yang sama, mencegah dominasi fitur tertentu.

* One-hot encoding fitur kategorikal menggunakan OneHotEncoder, karena model ML membutuhkan data kategorikal dalam format numerik.

* Menggabungkan preprocessing dengan ColumnTransformer agar bisa dilakukan sekaligus.

* Membagi data menjadi data latih dan uji (80:20) dengan stratifikasi, untuk menjaga proporsi kelas target agar tetap seimbang di kedua subset.

* Transformasi akhir dilakukan pada X_train dan X_test dengan fit_transform dan transform.

**Hasil:**

* Data siap untuk dilatih dalam bentuk matriks numerik.

* Tidak ada informasi bocor dari data uji ke data latih.
"""

# Memisahkan fitur (X) dan target (y) untuk memudahkan proses pemodelan
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Mendefinisikan fitur numerik dan kategorikal
# Ini penting agar preprocessing bisa dilakukan secara tepat pada masing-masing tipe data
num_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']
cat_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']

# Normalisasi fitur numerik agar skala seragam
num_transformer = StandardScaler()

# One-hot encoding untuk fitur kategorikal agar bisa diproses model
cat_transformer = OneHotEncoder(handle_unknown='ignore')

# Gabungkan preprocessing numerik dan kategorikal sekaligus
preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])

# Membagi dataset menjadi data latih (80%) dan data uji (20%) dengan stratifikasi berdasarkan target agar distribusi kelas tetap seimbang
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# Terapkan preprocessing ke data latih dan uji, lalu tampilkan ukuran hasilnya.
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print("Preprocessing selesai.")
print(f"Bentuk X_train_processed: {X_train_processed.shape}")
print(f"Bentuk X_test_processed: {X_test_processed.shape}")

"""## Modeling

Membangun dan membandingkan berbagai model klasifikasi untuk memilih model dengan performa terbaik.

**Model yang Digunakan:**

**Logistic Regression**

Model baseline sederhana untuk klasifikasi biner.

Alasannya dipilih:

* Cepat dan mudah digunakan.
* Memberikan baseline awal.
* Mudah diinterpretasi.
"""

# Model 1: Logistic Regression
# Membuat dan melatih model Logistic Regression dengan data yang sudah diproses untuk memprediksi penyakit jantung.
logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train_processed, y_train)

"""**Random Forest (Default)**

Model ensemble berbasis decision tree.

Alasannya dipilih:
* Mampu menangani data non-linear.
* Lebih tahan terhadap outlier dan overfitting.

"""

# Random Forest
# Membuat dan melatih model Random Forest untuk memprediksi penyakit jantung menggunakan data yang sudah diproses.
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_processed, y_train)

"""**Random Forest (Tuned)**
Melakukan tuning hyperparameter menggunakan GridSearchCV agar model bekerja lebih optimal.

Parameter yang dituning:
* n_estimators (jumlah pohon)
* max_depth (kedalaman maksimal pohon)
* min_samples_split (minimal data untuk split cabang)
"""

# Tuning Random Forest dengan Grid Search
# Melakukan pencarian kombinasi parameter terbaik Random Forest menggunakan GridSearchCV agar model lebih optimal.
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid_search.fit(X_train_processed, y_train)

best_rf = grid_search.best_estimator_
print(f"Best parameters RF: {grid_search.best_params_}")

"""**Hasil**

* Logistic Regression memberikan hasil awal yang layak, namun kalah performa dibanding Random Forest.

* Random Forest default cukup baik, tetapi setelah tuning, model meningkat terutama pada metrik recall dan F1.

## Evaluation

**Tujuan**
Menilai performa masing-masing model secara obyektif dan membandingkan efektivitasnya dalam prediksi.

**Metrik Evaluasi:**

* **Accuracy:** proporsi prediksi yang benar.
* **Precision:** proporsi positif yang benar-benar positif.
* **Recall:** proporsi kasus positif yang berhasil dideteksi.
* **F1-score:** harmonisasi antara precision dan recall.

**Proses Evaluasi:**

Digunakan fungsi *evaluate_model()* untuk semua model, mencetak classification report dan menghitung metrik utama.
"""

# Fungsi ini menghitung dan menampilkan metrik evaluasi utama serta laporan klasifikasi dari model pada data uji.
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(classification_report(y_test, y_pred))
    return acc, prec, rec, f1

# Evaluasi performa model Logistic Regression pada data uji.
print("Logistic Regression Evaluation")
logreg_metrics = evaluate_model(logreg, X_test_processed, y_test)

# Evaluasi performa model Random Forest standar pada data uji.
print("Random Forest Evaluation")
rf_metrics = evaluate_model(rf, X_test_processed, y_test)

# Evaluasi performa model Random Forest hasil tuning parameter pada data uji.
print("Tuned Random Forest Evaluation")
tuned_rf_metrics = evaluate_model(best_rf, X_test_processed, y_test)

"""### Hasil Evaluasi (Test Set)

| Model                 | Accuracy | Precision | Recall | F1-score |
|-----------------------|----------|-----------|--------|----------|
| Logistic Regression   | 0.86     | 0.84      | 0.89   | 0.86     |
| Random Forest Default | 0.88     | 0.87      | 0.90   | 0.88     |
| Random Forest Tuned   | 0.91     | 0.90      | 0.93   | 0.91     |

**Hasil**

* Random Forest Tuned paling unggul dalam akurasi dan recall.

* Model cukup andal untuk deteksi penyakit jantung secara dini.
"""

